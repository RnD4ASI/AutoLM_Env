# Test Run Documentation

## Test 1: KnowledgeService Execution

### Objective
To build vector, graph, and memory databases by processing two specific markdown documents: `APS112.md` and `APS113.md` using the `KnowledgeService`.

### Service Under Test
`src/pipeline/orchestration/knowledge_service.py`

### Input Files
*   `db/raw/au/standard/APS112.md`
*   `db/raw/au/standard/APS113.md`
*   Configuration: `config/main_config.json`
*   Hierarchy Data: `db/raw/hierarchy/APS_Hierarchy.csv` (used by the `hierarchy` chunking method, which is default)

### Test Script
`test_knowledge_service.py` (This script was generated by an automated worker to test the `KnowledgeService`.)

### Execution Summary
Direct execution of the test script failed. During the setup phase, an attempt to install required Python dependencies (e.g., pandas, sentence-transformers) resulted in an "OSError: [Errno 28] No space left on device". This prevented the script from running.

The *intended* execution flow, as defined in `test_knowledge_service.py`, is as follows:
1.  **Initialize `KnowledgeService`**: An instance of `KnowledgeService` is created using the configuration specified in `config/main_config.json`. Dummy input files (`APS112.md`, `APS113.md`) and a dummy hierarchy CSV (`APS_Hierarchy.csv`) are created if not present to ensure testability.
2.  **Process Documents**: The `process_documents()` method is called with `APS112.md` and `APS113.md`. This step is responsible for:
    *   Parsing the markdown files.
    *   Chunking the text content (likely using the hierarchy method based on `APS_Hierarchy.csv`).
    *   Generating vector embeddings for the chunks.
    *   Saving individual vector databases (Parquet format), graph databases (Pickle format), and episodic memory databases (Parquet format) for each document.
3.  **Merge Databases**:
    *   `merge_databases()`: Called to consolidate the individual vector databases into a single merged vector DB (Parquet) and individual graph databases into a single merged graph DB (Pickle).
    *   `_merge_memory_dbs()`: Called to consolidate individual episodic memory databases into a single merged episodic memory DB (Parquet).
4.  **Create Personality Memory**: The `_create_personality_memory_db()` method is called to generate a personality memory database (Parquet format) based on predefined sample traits.
5.  **Convert to JSON**: The `_convert_parquet_to_json()` method is called for each of the primary Parquet databases (merged vector, merged episodic memory, and personality memory) to create JSON versions suitable for downstream retriever components.

### Expected Artifacts (Simulated)
Based on the intended execution of `test_knowledge_service.py`:
*   Test Script: `test_knowledge_service.py`
*   Merged Vector Database:
    *   Parquet: `db/vector/v_au_standard_test.parquet`
    *   JSON: `db/vector/vector_db_au_standard_test_for_retriever.json`
*   Merged Graph Database:
    *   Pickle: `db/graph/g_au_standard_test.pkl`
*   Merged Episodic Memory Database:
    *   Parquet: `db/memory/episodic_memory_au_standard_test.parquet`
    *   JSON: `db/memory/episodic_memory_au_standard_test_for_retriever.json`
*   Personality Memory Database:
    *   Parquet: `db/memory/personality_memory.parquet` (as named in `KnowledgeService`, test script uses `personality_memory_au_standard_test_for_retriever.json` for its JSON version, the Parquet name in script is `personality_memory.parquet` as per `_create_personality_memory_db` default)
    *   JSON: `db/memory/personality_memory_au_standard_test_for_retriever.json`
*   Console logs: Detailed logs output to the console tracing the steps of initialization, document processing, database creation, merging, and conversion.

### Observed Outcome
Execution simulated due to environment error. The `KnowledgeService` was not actually run. The description above is based on the generated test script (`test_knowledge_service.py`) and a review of the `KnowledgeService` source code. The "OSError: [Errno 28] No space left on device" during dependency installation prevented any actual execution.

### Fixes Made
None, as the failure was environmental. The generated test script (`test_knowledge_service.py`) appears logically sound for its intended purpose of testing the `KnowledgeService` pipeline for the specified input documents.

## Test 2: PlanService Execution

### Objective
Generate a plan using `PlanService` for a hypothetical user goal.

### Service Under Test
`src/pipeline/orchestration/plan_service.py`

### Hypothetical Goal
"Generate a comprehensive overview of capital adequacy requirements as outlined in APS112 and APS113, including key definitions, calculation methods, and reporting obligations."

### Input Files (assumed dependencies)
*   `config/main_config.json`
*   `config/mle_config.json`
*   `config/task_prompt_library.json` (copied from `db/prompt/example_task_prompt_library.json`)
*   `config/meta_prompt_library.json` (copied from `db/prompt/example_meta_prompt_library.json`)

### Test Script
`test_plan_service.py` (This script was generated by an automated worker. Its specific content could not be directly retrieved for inclusion in this documentation turn, but its intended logic is described below.)

### Execution Summary
Direct execution of the test script failed due to an environment error: "OSError: [Errno 28] No space left on device" encountered during the installation of the `torch` Python dependency. The `torch` library is required by `src.pipeline.processing.generator.Generator`, which `PlanService` utilizes.

The *intended* execution flow, as described by the worker for `test_plan_service.py`, is as follows:
1.  **Initialize `PlanService`**: An instance of `PlanService` is created.
2.  **Define Hypothetical Goal**: The specific goal (mentioned above) is set.
3.  **Call `plan_task_sequence()`**: The `plan_task_sequence()` method of the `PlanService` instance is invoked with the defined goal.
4.  **Plan Generation (Intended Service Logic)**:
    *   The `PlanService`, if functional, would use its internal `AIUtility` (leveraging `config/meta_prompt_library.json`) to generate meta-prompts for selecting relevant task prompts from `config/task_prompt_library.json`.
    *   Further meta-prompts would be used to determine the sequence and dependencies of these selected task prompts.
    *   This process likely involves interaction with a Language Model (via the `Generator` class) to interpret prompts and make decisions.
5.  **Save Shortlisted Prompts**: The selected task prompts and their details would be saved to `config/shortlisted_prompts.json`.
6.  **Save Plan**: The final generated plan, structured as a graph of tasks and their dependencies (conforming to a defined schema like `db/schema/schema_plan.json`), would be saved to `config/prompt_flow_config.json`.

### Expected Artifacts (Simulated)
Based on the intended execution of `test_plan_service.py`:
*   Test Script: `test_plan_service.py`
*   Shortlisted Prompts: `config/shortlisted_prompts.json` (containing a list or dictionary of task prompts chosen from the library that are relevant to the goal)
*   Generated Plan: `config/prompt_flow_config.json` (containing the graph structure of the plan, including nodes representing tasks and edges representing dependencies, along with metadata)
*   Console Logs: Output including the hypothetical goal, confirmation of plan generation, and the paths (and ideally contents) of the `prompt_flow_config.json` and `shortlisted_prompts.json` files.

### Observed Outcome
Execution simulated due to environment error (disk space issue preventing `torch` installation). The `PlanService` was not actually run. The description above is based on the worker's report on the generated test script (`test_plan_service.py`) and a review of the `PlanService` source code.

### Fixes Made
None, as the failure was environmental. The described test script logic, based on the worker's report, appears sound for its intended purpose of testing the `PlanService` for plan generation.

## Test 3: ExecutionService Execution

### Objective
Execute a predefined plan (task prompts) using `ExecutionService`.

### Service Under Test
`src/pipeline/orchestration/execution_service.py`

### Input Files (Dummy/Mocked by Worker)
*   `config/prompt_flow_config.json` (A simple dummy plan, e.g., using `prompt_0003` - Data Analysis, created by the worker).
*   `config/shortlisted_prompts.json` (Dummy, created by worker).
*   Dummy JSON database files (mocking vector, graph, memory DBs for context retrieval, created by worker - specific names not available).
*   `config/task_prompt_library.json` (copied from `db/prompt/example_task_prompt_library.json`).
*   `config/main_config.json`, `config/mle_config.json` (assumed dependencies).

### Test Script
`test_execution_service.py` (This script was generated by an automated worker. Its specific content could not be directly retrieved for inclusion in this documentation turn, but its intended logic is described below.)

### Execution Summary
Direct execution of the test script failed due to an environment error: "ModuleNotFoundError: No module named 'torch._C'". This error resulted from an incomplete `torch` installation, which was caused by a prior "OSError: [Errno 28] No space left on device" during dependency installation.

The *intended* execution flow, based on the worker's description of `test_execution_service.py`, is as follows:
1.  **Initialize `ExecutionService`**: An instance of `ExecutionService` is created with paths to the dummy plan (`config/prompt_flow_config.json`), the (dummy) shortlisted prompts (`config/shortlisted_prompts.json`), and other configurations. It would also be set up to use the mocked database paths.
2.  **Call `execute_plan()`**: The `execute_plan()` method of the `ExecutionService` instance is invoked.
3.  **Iterate Through Plan Tasks**: The service, if functional, would parse the `prompt_flow_config.json` and iterate through the defined tasks (nodes).
4.  **Context Retrieval**: For each task, it would attempt to retrieve relevant context from the (mocked) vector, graph, and memory databases using its retriever components.
5.  **Task Execution**: The core of each task involves using a generator (specifically `src.pipeline.processing.generator.Generator`, which requires `torch`) to process the task prompt along with the retrieved context. This would involve selecting appropriate models and parameters based on `config/mle_config.json`.
6.  **Collect Results**: The output from each task execution (e.g., generated text, data, or status) would be collected.

### Expected Artifacts (Simulated)
Based on the intended execution of `test_execution_service.py`:
*   Test Script: `test_execution_service.py`
*   The dummy `config/prompt_flow_config.json` used as input.
*   The dummy `config/shortlisted_prompts.json` used as input.
*   The dummy JSON database files used for mocked context retrieval.
*   Console output: Detailed logs showing the initialization of `ExecutionService`, the tasks being processed from the plan, any context retrieved (which would be from the dummy DBs), the (simulated) execution of each task prompt, and the final results or error messages for each task.

### Observed Outcome
Execution simulated due to environment error (disk space issue preventing complete `torch` installation, leading to `ModuleNotFoundError: No module named 'torch._C'`). The `ExecutionService` was not actually run. The description above is based on the worker's report on the generated test script, dummy data creation, and code review of `src/pipeline/orchestration/execution_service.py`.

### Fixes Made
None, as the failure was environmental. The described test script logic and dummy data setup appear sound for their intended purpose of testing the `ExecutionService` with a predefined plan.
